{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "count=20\n",
    "data=pd.read_csv(f'cluster/hourly_{count}.csv')\n",
    "\n",
    "data=data.drop(columns=['station_id','1小时雨量','6小时雨量','date_y','is_holiday'])\n",
    "\n",
    "\n",
    "data = data.dropna()  \n",
    "X = data.drop(columns=['date', 'PM25_Concentration'])  # 事件变量\n",
    "y = data['PM25_Concentration']  \n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [100],\n",
    "    'reg_alpha': [0.1],  \n",
    "    'reg_lambda': [0.9]   \n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.2f} MSE: {mse:.2f} MAE: {mae:.2f}, R²: {r2:.2f}\")\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'event': X.columns,\n",
    "    'shap_impact': np.abs(shap_values.values).mean(axis=0),\n",
    "    'direction': np.where(shap_values.values.mean(axis=0) > 0, '+', '-')\n",
    "}).sort_values('shap_impact', ascending=False)\n",
    "shap_values_full = explainer(data.drop(columns=['date', 'PM25_Concentration']))\n",
    "print(\"Top 30 影响事件:\")\n",
    "print(shap_df.head(31))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_columns = [\n",
    "    \"Traffic Accidents\", \"Transportation Construction and Development\",\n",
    "    \"Transportation Rehabilitation and Adjustment\", \"Traffic Control\",\n",
    "    \"Humanities Activities\", \"Urban Management\", \"Large Social Events\",\n",
    "    \"Large Examinations\", \"Accidents\", \"Extreme Weather\", \"Temperature Changes\",\n",
    "    \"Meteorological Transformations\", \"Meteorological Publicity\",\n",
    "    \"Epidemic Prevention and Control\", \"Adjustment of urban operations\",\n",
    "    \"Disease Outbreaks\", \"Social Security Incidents\", \"Air Pollution\",\n",
    "    \"Aviation Safety Incidents\", \"Illegal Incidents\"\n",
    "]\n",
    "\n",
    "event_df = shap_df[shap_df['event'].isin(event_columns)]\n",
    "\n",
    "event_df['impact_ratio'] = event_df['shap_impact'] / event_df['shap_impact'].sum()\n",
    "\n",
    "filtered_shap_df = event_df.sort_values(by='impact_ratio', ascending=False)\n",
    "\n",
    "print(\"\\n每个指定事件在所有事件中影响占比：\")\n",
    "for idx, row in filtered_shap_df.iterrows():\n",
    "    print(f\"{row['event']}: {row['impact_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "count=100\n",
    "data=pd.read_csv(f'cluster/hourly_{count}.csv')\n",
    "data = data[data['station_id'] == '1001A']\n",
    "\n",
    "data=data.drop(columns=['station_id','1小时雨量','6小时雨量'])\n",
    "\n",
    "data = data.dropna()  \n",
    "X = data.drop(columns=['date', 'PM25_Concentration'])  # 事件变量\n",
    "y = data['PM25_Concentration']  \n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [100],\n",
    "    'reg_alpha': [0.1], \n",
    "    'reg_lambda': [0.9]  \n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.2f} MSE: {mse:.2f} MAE: {mae:.2f}, R²: {r2:.2f}\")\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer(X_test)\n",
    "shap_df = pd.DataFrame({\n",
    "    'event': X.columns,\n",
    "    'shap_impact': np.abs(shap_values.values).mean(axis=0),\n",
    "    'direction': np.where(shap_values.values.mean(axis=0) > 0, '+', '-')\n",
    "}).sort_values('shap_impact', ascending=False)\n",
    "shap_values_full = explainer(data.drop(columns=['date', 'PM25_Concentration']))\n",
    "print(\"Top 30 影响事件:\")\n",
    "print(shap_df.head(31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_columns = [\n",
    "    \"Haze Weather\",\n",
    "    \"Thunderstorm Weather\",\n",
    "    \"Epidemic Prevention\",\n",
    "    \"COVID-19 Epidemic Prevention\",\n",
    "    \"High Temperature Weather\",\n",
    "    \"Work Resumption and Production Recovery\",\n",
    "    \"Cold Wave Weather\",\n",
    "    \"Heating Supply Activities\",\n",
    "    \"COVID-19 New Cases\",\n",
    "    \"Public Order Issues\",\n",
    "    \"Heavy Rain Weather\",\n",
    "    \"Campus Incidents\",\n",
    "    \"Frequent Respiratory Infections\",\n",
    "    \"Cultural Activities\",\n",
    "    \"Criminal Cases\",\n",
    "    \"Pandemic Intensification\",\n",
    "    \"Traffic Accidents\",\n",
    "    \"Traffic Adjustment and Optimization\",\n",
    "    \"College Entrance Exam\",\n",
    "    \"Performances\"\n",
    "]\n",
    "\n",
    "# 筛选出指定事件列\n",
    "event_df = shap_df[shap_df['event'].isin(event_columns)]\n",
    "\n",
    "# 计算每个事件的影响占比\n",
    "event_df['impact_ratio'] = event_df['shap_impact'] / event_df['shap_impact'].sum()\n",
    "\n",
    "# 按照影响占比降序排序\n",
    "filtered_shap_df = event_df.sort_values(by='impact_ratio', ascending=False)\n",
    "\n",
    "# 输出结果\n",
    "print(\"\\n每个指定事件在所有事件中影响占比：\")\n",
    "for idx, row in filtered_shap_df.iterrows():\n",
    "    print(f\"{row['event']}: {row['impact_ratio']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features=event_columns\n",
    "feature_indices = [X.columns.get_loc(col) for col in selected_features]\n",
    "shap_values_selected = shap_values[:, feature_indices]\n",
    "X_test_selected = X_test.iloc[:, feature_indices]\n",
    "shap.summary_plot(shap_values_selected, \n",
    "                 X_test_selected, \n",
    "                 plot_type=\"bar\", \n",
    "                 feature_names=selected_features,\n",
    "                 show=False)  \n",
    "plt.xlabel('mean|SHAP|')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(shap_values_selected, \n",
    "                 X_test_selected, \n",
    "                 feature_names=selected_features,\n",
    "                 show=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.columns)\n",
    "for feature in event_columns:\n",
    "    if feature in X_test.columns:  # 确保特征存在\n",
    "        shap.dependence_plot(feature, shap_values.values, X_test, interaction_index=None)\n",
    "    else:\n",
    "        print(f\"警告: 特征 {feature} 不在 X_test 中，跳过绘制\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mumu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
