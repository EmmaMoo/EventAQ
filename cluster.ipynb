{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df_cluster=pd.read_csv('list.csv')\n",
    "event_list = df_cluster['事件名称'].tolist()\n",
    "print(df_cluster['事件名称'])\n",
    "print(event_list)\n",
    "print(len(event_list))\n",
    "\n",
    "model_path = 'models/bge-large-zh-v1.5'#改成你的模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model_config = BertConfig.from_pretrained(model_path)\n",
    "model_config.output_hidden_states = True\n",
    "model_config.output_attentions = True\n",
    "bert_model = BertModel.from_pretrained(model_path, config=model_config)\n",
    "\n",
    "input_ids = tokenizer(event_list, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids=input_ids)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现层次聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_LEVELS = [2000, 500, 100, 20, 2]\n",
    "cluster_data = pd.DataFrame({'事件名称': event_list, '向量': list(embeddings)})\n",
    "\n",
    "iteration = 0  \n",
    "for current_k in CLUSTER_LEVELS:\n",
    "    print(f\"正在执行第 {iteration} 层聚类，K={current_k}\")\n",
    "\n",
    "    vectors = np.vstack(cluster_data['向量'].values)\n",
    "    kmeans = KMeans(n_clusters=current_k, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(vectors)\n",
    "    cluster_data['聚类类别'] = cluster_labels\n",
    "\n",
    "    file_name = f\"cluster_level_{iteration}.csv\"\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_data['向量'] = cluster_data['聚类类别'].map(lambda c: cluster_centers[c])\n",
    "\n",
    "    iteration += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pca = PCA(n_components=3)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "fig = plt.figure(figsize=(18, 16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(\n",
    "    reduced_embeddings[:, 0], \n",
    "    reduced_embeddings[:, 1], \n",
    "    reduced_embeddings[:, 2], \n",
    "    c=cluster_labels, \n",
    "    cmap='viridis', \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Cluster Labels\")\n",
    "ax.set_title('BERT Embeddings - KMeans Clustering')\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in range(5):\n",
    "    file_path = f\"cluster_level_{i}.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    agg_df = df.groupby('聚类类别')['事件名称'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "    agg_df.rename(columns={'聚类类别': '主题', '事件名称': '主题词'}, inplace=True)\n",
    "    agg_df['主题'] = agg_df['主题'].astype(str)\n",
    "    agg_df = agg_df.sort_values(by='主题')\n",
    "    agg_df['主题'] = agg_df['主题'].astype(int)\n",
    "    agg_df = agg_df.sort_values(by='主题')\n",
    "\n",
    "    output_path = f\"{i}_cluster_summary.csv\"\n",
    "    agg_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"转换完成，结果已保存至:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为每一个类别加标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "count=500\n",
    "client = ZhipuAI(api_key=\"Your_key\")  # 替换为你的API密钥\n",
    "def generate_event(i, content):\n",
    "\n",
    "    content_str = ', '.join(content) if isinstance(content, list) else str(content)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"GLM-4-Air\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"给出以下社会活动或气象活动事件的共同主题，只给出一个主题：{content_str}。要求输出主题标签，主题标签也是事件名称，只输出一个标签，字数要求少于10个字，不要解释，不要有特殊字符\n",
    "                 示例：\n",
    "                 输入：\"滑雪场客流量大, 部分桥区节点行驶缓慢, 景区被评为5A级景区, 城区环路、联络线交通流量大, 医院是交通热点, 景点周边大车流, 游客不文明拍照行为, 清明小长假游客增多, 看玻璃栈道游客车辆拥堵, 黄牛炒卖门票, 部分商圈周边车流量大\"\n",
    "                 输出：\"游客增多\"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        output_text = response.choices[0].message.content  \n",
    "        print(output_text)\n",
    "        return i, output_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}: {e}\")\n",
    "        return i, None\n",
    "\n",
    "# 读取输入数据\n",
    "input_path = f'clusters_label_{count}.csv'\n",
    "output_path = f'clusters_label_{count}.csv'\n",
    "\n",
    "df_label = pd.read_csv(input_path)\n",
    "event_list = df_label['主题词'].tolist()\n",
    "    \n",
    "results = []  \n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = [\n",
    "        executor.submit(generate_event, idx, content) for idx, content in zip(df_label.index, event_list)\n",
    "    ]\n",
    "    for future in as_completed(futures):\n",
    "        idx, output_text = future.result()\n",
    "        if output_text is not None:\n",
    "            results.append({'index': idx, 'label': output_text})\n",
    "\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "df_label = df_label.merge(results_df, left_index=True, right_on='index', how='left')\n",
    "df_label = df_label[['数量', 'label_id', 'label', '主题词', 'index']]\n",
    "df_label.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_path = \"cluster/\"\n",
    "file_names = [f\"cluster_level_{i}.csv\" for i in range(5)] \n",
    "output_file = base_path + \"merged_clusters.csv\"  \n",
    "df_label=pd.read_csv('clusters_label_20.csv')\n",
    "df_label_=pd.read_csv('cluster/clusters_label_100.csv')\n",
    "df_label_500=pd.read_csv('cluster/clusters_label_500.csv')\n",
    "\n",
    "dfs = []\n",
    "for i, file_name in enumerate(file_names, start=1):\n",
    "    file_path = base_path + file_name\n",
    "    df = pd.read_csv(file_path) \n",
    "    df = df.rename(columns={\"聚类类别\": f\"聚类类别{i}\"}) \n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=\"事件名称\", how=\"inner\") \n",
    "\n",
    "merged_df['聚类类别5'] = merged_df['聚类类别5'].map({0: \"社会活动事件\", 1: \"气象活动事件\"})\n",
    "for i in range(merged_df.shape[0]):\n",
    "    merged_df['聚类类别4'][i] = df_label['label'][df_label['label_id'] == merged_df['聚类类别4'][i]].values[0]\n",
    "for i in range(merged_df.shape[0]):\n",
    "    merged_df['聚类类别3'][i] = df_label_['label'][df_label_['label_id'] == merged_df['聚类类别3'][i]].values[0]\n",
    "for i in range(merged_df.shape[0]):\n",
    "    merged_df['聚类类别2'][i] = df_label_500['label'][df_label_500['label_id'] == merged_df['聚类类别2'][i]].values[0]\n",
    "# 保存最终合并的文件\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"合并完成，文件已保存至 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每类事件的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count=20\n",
    "df = pd.read_csv(f'cluster/news_lable_{count}.csv')\n",
    "event_time_counts = df.groupby('标签')['时间'].nunique().reset_index()\n",
    "event_time_counts = df_label.merge(event_time_counts, left_on='label', right_on='标签', how='left')\n",
    "event_time_counts.rename(columns={'时间': '数量'}, inplace=True)\n",
    "event_time_counts = event_time_counts.sort_values(by='数量', ascending=False)\n",
    "event_time_counts = event_time_counts[['数量', 'label', '主题词', '标签']]\n",
    "event_time_counts.to_csv(f'cluster/clusters_label_{count}.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"统计结果已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "count=20\n",
    "df_lable = pd.read_csv(f'cluster/news_lable_{count}.csv')\n",
    "\n",
    "df_lable = df_lable.dropna(subset=['标签'])\n",
    "unique_times = df_lable['时间'].unique()\n",
    "unique_events = df_lable['标签'].unique()\n",
    "\n",
    "time_event_pairs = list(itertools.product(unique_times, unique_events))\n",
    "\n",
    "df_time_event = pd.DataFrame(time_event_pairs, columns=['时间', '标签'])\n",
    "event_counts = df_lable.groupby(['时间', '标签']).size().reset_index(name='事件数量')\n",
    "df_time_event = df_time_event.merge(event_counts, on=['时间', '标签'], how='left')\n",
    "\n",
    "df_time_event['事件数量'] = df_time_event['事件数量'].fillna(0)\n",
    "\n",
    "pivot_table = df_time_event.pivot(index='时间', columns='标签', values='事件数量')\n",
    "\n",
    "print(pivot_table)\n",
    "pivot_table.to_csv(f'cluster/hourly_{count}.csv')\n",
    "print(pivot_table.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count=20\n",
    "df_time=pd.read_csv(f'cluster/hourly_{count}.csv')\n",
    "df_qx = pd.read_csv('获取的气象数据.csv')#改为你的气象数据文件路径\n",
    "\n",
    "df_time['date'] = pd.to_datetime(\n",
    "    df_time['date'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})', expand=False),\n",
    "    errors='coerce'\n",
    ")\n",
    "df_time = df_time.dropna(subset=['date']).copy()\n",
    "\n",
    "event_cols = df_time.columns[1:]\n",
    "df_time[event_cols] = df_time[event_cols].clip(upper=1.0)\n",
    "\n",
    "\n",
    "print(df_qx.columns)\n",
    "df_qx['date'] = pd.to_datetime(df_qx['date'], errors='coerce')\n",
    "df_qx = df_qx.dropna(subset=['date']).copy()\n",
    "\n",
    "\n",
    "df_qx['date_key'] = df_qx['date'].dt.normalize() \n",
    "\n",
    "\n",
    "df_time = df_time.set_index('date')\n",
    "\n",
    "\n",
    "merged = df_qx.merge(\n",
    "    df_time,\n",
    "    left_on='date_key',\n",
    "    right_index=True,\n",
    "    how='left',\n",
    "    suffixes=('', '_event')\n",
    ")\n",
    "\n",
    "merged[event_cols] = merged[event_cols].fillna(0)\n",
    "\n",
    "\n",
    "merged.drop(columns=['date_key'], inplace=True)\n",
    "\n",
    "\n",
    "output_path = f'cluster/hourly_{count}.csv'\n",
    "merged.to_csv(output_path, index=False)\n",
    "print(len(merged.columns))\n",
    "print(f\"合并后的数据已保存到 {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mumu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
